# configs/gpt2_small.yaml
model:
  block_size: 128
  vocab_size: 50257
  n_layer: 4
  n_head: 1
  n_embd: 768
  dropout: 0.1
  bias: False
  max_new_token: 20
  temperature: 0.8 
  top_k: 4

data:
  train_path: "data/raw/train.txt"
  val_path:   "data/raw/val.txt"
  batch_size: 2
  # um_workers: 4

train:
   device: 'cpu'
   learning_rate: 0.0006
   weight_decay: 0.1
   max_steps: 10
   accum_steps: 2
